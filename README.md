# RAG Copilot for IT & Security Knowledge

## Project status
**Current state:** documentation & architecture only (Phase 0).  
There is **no runnable MVP yet**.

**What exists today:**
- Enterprise-grade architecture (`docs/architecture.md`)
- Feature design: Strict Mode, Answer Packs, Audit/Export, Staleness Radar
- Project workflow and milestones (`docs/project-management.md`)

**What will exist in MVP (Milestone M1):**
- A local CLI to ingest a small corpus and answer with **ranked citations**
- Strict Mode returning **Not found** when evidence is missing

An internal **documentation copilot + RAG (Retrieval-Augmented Generation)** for **IT / Support / Helpdesk / Cybersecurity** teams.

**Core promise:** answers are **evidence-based**, with **mandatory citations**, source ranking, and a **Strict Mode** that refuses to guess.

---

## Product positioning
RAG Copilot is built for operational teams that need **auditable answers** and **actionable outputs**—not generic chat.

- **For Support (L1/L2):** fast, consistent ticket-ready responses grounded in internal runbooks.
- **For Security (SecOps):** evidence-backed guidance you can justify and trace.
- **For Knowledge owners:** visibility into what’s used, what’s missing, and what’s stale.

---

## Why this project exists
Internal knowledge is usually scattered across PDFs, wikis, shared drives, and ticket exports. It becomes:
- hard to search,
- easy to forget,
- risky to trust,
- and expensive to maintain.

RAG Copilot turns internal docs into a **controlled knowledge interface** with traceability and governance.

---

## Key capabilities (high level)
### Evidence-first answers (anti-hallucination)
- **Mandatory citations** in every answer
- **Ranked sources** (top-k retrieval + relevance ordering)
- **Strict Mode:** if evidence is insufficient → returns *“Not found in knowledge base”*

### “Answer Packs” (on-demand)
When requested, the copilot generates an **Answer Pack**:
- Executive summary (30 seconds)
- Step-by-step runbook
- Checklist
- Ticket response template (Helpdesk style)
- Optional: risks / impact / rollback notes (when relevant)

> Answer Packs are **opt-in**: available as a clear option, not generated by default.

### Staleness Radar (admin-facing, MVP-light)
Detects candidates for updates, e.g.:
- references to outdated versions / EOL products
- conflicting guidance across versions
- heavily used docs with low retrieval confidence

### Audit / Export mode
Export an answer with its citations and sources for:
- attaching to tickets,
- incident reports,
- internal reviews.

---

## Local-first (no paid APIs)
This project is designed to run **without paid LLM APIs**, using:
- a local LLM runtime (e.g., Ollama)
- open-source models (e.g., Llama / Mistral family)

The repository documentation and architecture will remain **vendor-optional**, but the default path is **no external paid inference**.

---

## Repository structure (planned)
- `/docs/` product and design docs
- `/eval/` evaluation datasets and reports (added later)
- `/demo/` demo scripts/videos (added later)
- (later) `/backend/`, `/frontend/`, `/infra/`

---

## MVP scope and principles
- MVP definition: `/docs/mvp-scope.md`
- System principles: `/docs/system-principles.md`
- Roadmap: `/docs/roadmap.md`

---

## Non-goals (explicit)
To keep the project realistic and professional, the MVP will NOT include:
- full production connectors (ServiceNow/Jira/Confluence/SharePoint) from day one
- SSO/SAML enterprise auth in MVP
- multi-tenant architecture
- “creative writing” answers without evidence
- auto-editing internal documentation (only suggestions/flags in MVP)
- direct ticket creation in real systems (export templates only)

---

## Project status
**Current phase:** Initial documentation and project definition.  
**Next milestone:** Architecture spec and evaluation plan.
